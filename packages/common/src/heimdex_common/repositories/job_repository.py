"""
Repository for Job and JobEvent Data Access Logic.

This module defines the `JobRepository`, which serves as a dedicated data
access layer for the `Job` and `JobEvent` models. It implements the Repository
Pattern, which means it encapsulates all the SQLAlchemy-specific query and
data manipulation logic. This creates a clean separation of concerns, where the
rest of the application can interact with job-related data through a simple,
domain-focused API, without needing to know about the underlying database details.
"""

from __future__ import annotations

import uuid
from collections.abc import Sequence
from datetime import UTC, datetime
from typing import Any

from sqlalchemy import desc, func, select
from sqlalchemy.orm import Session, selectinload

from ..models import Job, JobEvent, JobStatus, Outbox


class JobRepository:
    """
    Manages all database operations for the `Job` and `JobEvent` models.

    This repository provides a high-level, abstracted interface for all
    Create, Read, Update, and Delete (CRUD) operations related to jobs. By
    channeling all data access through this class, we ensure that business
    rules are consistently applied, and the data remains in a valid state.

    An instance of this class is typically created with a specific SQLAlchemy
    `Session` object, meaning that all operations performed by a single
    repository instance will be part of the same database transaction.
    """

    def __init__(self, session: Session):
        """
        Initializes the repository with a specific database session.

        The repository is designed to be short-lived, typically created for a
        single request or unit of work. It is injected with an active SQLAlchemy
        session, which it uses to execute all its database operations.

        Args:
            session: An active SQLAlchemy session. All operations within this
                     repository instance will be bound to this session's
                     transaction.
        """
        self.session = session

    def create_job(
        self,
        org_id: uuid.UUID,
        job_type: str,
        idempotency_key: str | None = None,
        requested_by: str | None = None,
        priority: int = 0,
    ) -> Job:
        """
        Atomically creates a new job and its initial creation event.

        This method encapsulates the logic for creating a new `Job` in the
        `QUEUED` state. Crucially, it also creates the first `JobEvent`,
        ensuring that the job's audit trail begins at the moment of creation.

        Args:
            org_id: The UUID of the organization this job belongs to.
            job_type: A string identifier for the type of work to be done.
            idempotency_key: An optional, client-provided key that, when used
                             with the unique constraint in the `Job` model,
                             prevents the creation of duplicate jobs.
            requested_by: Optional metadata about the user or service that
                          initiated the job.
            priority: The job's priority (higher values are processed first).

        Returns:
            The newly created and persisted `Job` instance.

        Raises:
            sqlalchemy.exc.IntegrityError: If the `idempotency_key` is not
                unique for the given `org_id`, the database will raise this
                error, which the calling service can handle gracefully.
        """
        job = Job(
            id=uuid.uuid4(),
            org_id=org_id,
            type=job_type,
            status=JobStatus.QUEUED,
            idempotency_key=idempotency_key,
            requested_by=requested_by,
            priority=priority,
        )
        self.session.add(job)
        # We flush the session here to ensure the job's ID is generated by the
        # database and available for the subsequent event logging, all within
        # the same transaction.
        self.session.flush()

        self.log_job_event(
            job_id=job.id,
            prev_status=None,
            next_status=JobStatus.QUEUED.value,
        )

        return job

    def create_job_with_outbox(
        self,
        org_id: uuid.UUID,
        job_type: str,
        job_key: str,
        task_name: str,
        payload: dict[str, Any],
        requested_by: str | None = None,
        priority: int = 0,
    ) -> Job:
        """
        Creates a job with transactional outbox pattern for exactly-once delivery.

        This method implements the transactional outbox pattern by atomically
        writing both the job record and the outbox message within the same
        database transaction. This eliminates the split-brain problem where a
        job could exist in the database but never be published to the queue.

        The method is idempotent: if a job with the same job_key already exists,
        it returns the existing job instead of creating a duplicate.

        Args:
            org_id: The UUID of the organization this job belongs to.
            job_type: The type of job (e.g., "video_ingest", "mock_process").
            job_key: The deterministic server-side idempotency key (SHA256 hash).
            task_name: The name of the Dramatiq actor to invoke.
            payload: The JSONB payload containing message arguments.
            requested_by: Optional metadata about the user or service.
            priority: The job's priority (higher values processed first).

        Returns:
            The created (or existing) Job instance.
        """
        # Check if a job with this job_key already exists (idempotency check)
        existing_job = self.get_job_by_job_key(job_key)
        if existing_job:
            return existing_job

        # Create the job
        job = Job(
            id=uuid.uuid4(),
            org_id=org_id,
            type=job_type,
            status=JobStatus.QUEUED,
            job_key=job_key,
            requested_by=requested_by,
            priority=priority,
        )
        self.session.add(job)
        self.session.flush()  # Get the job.id

        # Log the initial event
        self.log_job_event(
            job_id=job.id,
            prev_status=None,
            next_status=JobStatus.QUEUED.value,
        )

        # Write to outbox in the same transaction
        outbox_message = Outbox(
            job_id=job.id,
            task_name=task_name,
            payload=payload,
        )
        self.session.add(outbox_message)

        return job

    def get_job_by_job_key(self, job_key: str) -> Job | None:
        """
        Retrieves a job by its deterministic job_key.

        This is used for server-side idempotency checks to ensure that the same
        logical job is not created multiple times.

        Args:
            job_key: The SHA256 hash representing the job's idempotency key.

        Returns:
            The Job instance if found, None otherwise.
        """
        stmt = select(Job).where(Job.job_key == job_key)
        return self.session.execute(stmt).scalar_one_or_none()

    def get_unsent_outbox_messages(self, limit: int = 100) -> Sequence[Outbox]:
        """
        Retrieves unsent outbox messages for the outbox dispatcher.

        Uses FOR UPDATE SKIP LOCKED to allow concurrent dispatcher instances
        to process different batches without blocking each other.

        Args:
            limit: Maximum number of messages to retrieve.

        Returns:
            A sequence of unsent Outbox records.
        """
        stmt = (
            select(Outbox)
            .where(Outbox.sent_at.is_(None))
            .order_by(Outbox.created_at)
            .limit(limit)
            .with_for_update(skip_locked=True)
        )
        return self.session.execute(stmt).scalars().all()

    def mark_outbox_sent(self, outbox_id: int) -> None:
        """
        Marks an outbox message as successfully sent.

        Args:
            outbox_id: The ID of the outbox record to mark as sent.
        """
        stmt = select(Outbox).where(Outbox.id == outbox_id)
        outbox = self.session.execute(stmt).scalar_one_or_none()
        if outbox:
            outbox.sent_at = datetime.now(UTC)

    def mark_outbox_failed(self, outbox_id: int, error: str) -> None:
        """
        Records a failed publish attempt for an outbox message.

        Args:
            outbox_id: The ID of the outbox record.
            error: The error message from the failed publish attempt.
        """
        stmt = select(Outbox).where(Outbox.id == outbox_id)
        outbox = self.session.execute(stmt).scalar_one_or_none()
        if outbox:
            outbox.fail_count += 1
            outbox.last_error = error[:2048]  # Truncate to column length

    def get_job_by_id(self, job_id: uuid.UUID) -> Job | None:
        """Retrieves a single job by its primary key."""
        stmt = select(Job).where(Job.id == job_id)
        return self.session.execute(stmt).scalar_one_or_none()

    def get_job_with_events(self, job_id: uuid.UUID) -> Job | None:
        """
        Retrieves a job and eagerly loads its entire event history.

        This method is useful when you need to display the full lifecycle of a
        job. It uses `selectinload` to issue a second, efficient query to fetch
        all related events, avoiding the "N+1 query problem".

        Args:
            job_id: The UUID of the job to retrieve.

        Returns:
            The `Job` instance with its `events` attribute pre-populated, or
            `None` if not found.
        """
        stmt = select(Job).options(selectinload(Job.events)).where(Job.id == job_id)
        return self.session.execute(stmt).scalar_one_or_none()

    def get_latest_job_event(self, job_id: uuid.UUID) -> JobEvent | None:
        """Efficiently retrieves only the most recent event for a given job."""
        stmt = (
            select(JobEvent).where(JobEvent.job_id == job_id).order_by(desc(JobEvent.ts)).limit(1)
        )
        return self.session.execute(stmt).scalar_one_or_none()

    def update_job_status(
        self,
        job_id: uuid.UUID,
        status: JobStatus,
        last_error_code: str | None = None,
        last_error_message: str | None = None,
        log_event: bool = True,
        event_detail: dict[str, Any] | None = None,
    ) -> None:
        """
        Updates a job's status and logs a corresponding state transition event.

        This is a critical transactional method. It ensures that any change to
        a job's state is recorded in the immutable `JobEvent` log.

        Args:
            job_id: The ID of the job to update.
            status: The new `JobStatus` for the job.
            last_error_code: A machine-readable error code, if the job failed.
            last_error_message: A human-readable error message.
            log_event: If `True`, a `JobEvent` is created for the transition.
            event_detail: Rich, structured data to include in the event log.

        Raises:
            ValueError: If the specified job does not exist.
        """
        job = self.get_job_by_id(job_id)
        if not job:
            raise ValueError(f"Job {job_id} not found")

        prev_status = job.status
        if prev_status == status:
            return  # No status change, nothing to do

        job.status = status
        job.updated_at = datetime.now(UTC)
        job.last_error_code = last_error_code
        job.last_error_message = last_error_message[:2048] if last_error_message else None

        if status == JobStatus.RUNNING and job.started_at is None:
            job.started_at = datetime.now(UTC)
        if status in {
            JobStatus.SUCCEEDED,
            JobStatus.FAILED,
            JobStatus.CANCELED,
            JobStatus.DEAD_LETTER,
        }:
            job.finished_at = datetime.now(UTC)

        if log_event:
            self.log_job_event(
                job_id=job_id,
                prev_status=prev_status.value,
                next_status=status.value,
                detail_json=event_detail,
            )

    def update_job_with_stage_progress(
        self,
        job_id: uuid.UUID,
        stage: str,
        progress: int,
        status: JobStatus | None = None,
    ) -> None:
        """
        Logs a progress update event without necessarily changing the job's main status.

        This is useful for long-running jobs that have multiple internal stages
        (e.g., "downloading", "transcoding", "indexing"). It creates a `JobEvent`
        to record this progress without altering the job's primary `RUNNING` status.

        Args:
            job_id: The ID of the job to update.
            stage: A string describing the current processing stage.
            progress: A progress indicator, typically a percentage (0-100).
            status: An optional new status for the job. If `None`, the status
                    is not changed, and this is treated as a pure progress update.
        """
        job = self.get_job_by_id(job_id)
        if not job:
            raise ValueError(f"Job {job_id} not found")

        event_detail = {"stage": stage, "progress": progress}

        if status and job.status != status:
            self.update_job_status(job_id, status, log_event=True, event_detail=event_detail)
        else:
            # If status is not changing, just log a progress event.
            self.log_job_event(
                job_id=job_id,
                prev_status=job.status.value,
                next_status=job.status.value,
                detail_json=event_detail,
            )

    def log_job_event(
        self,
        job_id: uuid.UUID,
        prev_status: str | None,
        next_status: str,
        detail_json: dict[str, Any] | None = None,
    ) -> JobEvent:
        """
        Creates and persists a new `JobEvent` record.

        This method provides a direct way to add an entry to the immutable
        audit log for a job.

        Returns:
            The newly created `JobEvent` instance.
        """
        event = JobEvent(
            id=uuid.uuid4(),
            job_id=job_id,
            ts=datetime.now(UTC),
            prev_status=prev_status,
            next_status=next_status,
            detail_json=detail_json,
        )
        self.session.add(event)
        self.session.flush()
        return event

    def get_queued_jobs(
        self, org_id: uuid.UUID, limit: int = 10, job_type: str | None = None
    ) -> Sequence[Job]:
        """
        Fetches a batch of queued jobs, ready for processing by a worker.

        This is a critical query for the worker service. It selects jobs that are
        in the `QUEUED` state, ordering them by priority and then by creation
        time to ensure that high-priority and older jobs are processed first.

        Returns:
            A sequence of `Job` instances ready for processing.
        """
        stmt = (
            select(Job)
            .where(Job.org_id == org_id, Job.status == JobStatus.QUEUED)
            .order_by(desc(Job.priority), Job.created_at)
            .limit(limit)
        )
        if job_type:
            stmt = stmt.where(Job.type == job_type)
        return self.session.execute(stmt).scalars().all()

    def get_jobs_by_status(
        self, org_id: uuid.UUID, status: JobStatus, limit: int = 100
    ) -> Sequence[Job]:
        """Retrieves jobs for an organization, filtered by a specific status."""
        stmt = (
            select(Job)
            .where(Job.org_id == org_id, Job.status == status)
            .order_by(desc(Job.created_at))
            .limit(limit)
        )
        return self.session.execute(stmt).scalars().all()

    def get_job_statistics(self, org_id: uuid.UUID) -> dict[str, int]:
        """
        Calculates and returns job count statistics, grouped by status.

        This method performs an efficient database-level aggregation to count
        the number of jobs in each status category for a given organization. This
        is useful for building monitoring dashboards.

        Returns:
            A dictionary mapping status names to their respective counts.
        """
        stmt = (
            select(Job.status, func.count(Job.id)).where(Job.org_id == org_id).group_by(Job.status)
        )
        results = self.session.execute(stmt).all()
        return {status.value: count for status, count in results}

    def increment_attempt(self, job_id: uuid.UUID) -> Job:
        """
        Atomically increments the retry attempt counter for a job.

        This is typically called by a worker just before re-queueing a failed job.

        Returns:
            The updated `Job` instance.
        """
        job = self.get_job_by_id(job_id)
        if not job:
            raise ValueError(f"Job {job_id} not found")

        job.attempt += 1
        job.updated_at = datetime.now(UTC)
        self.session.flush()
        return job
